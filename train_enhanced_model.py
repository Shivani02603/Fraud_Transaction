#!/usr/bin/env python3
"""
Enhanced Fraud Detection Model Training Script with Optimized Features

This script implements the requested enhanced features:
1. Transaction Frequency Features
2. Merchant/Geographic Pattern Features
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import pickle
import warnings
warnings.filterwarnings('ignore')

def create_optimized_features(df):
    """Create optimized enhanced features for fraud detection"""
    print("Creating optimized enhanced features...")
    
    # Sort by nameOrig and step for efficient processing
    df = df.sort_values(['nameOrig', 'step']).reset_index(drop=True)
    
    # Transaction frequency features (optimized calculation)
    print("  - Transaction frequency features...")
    user_stats = df.groupby('nameOrig').agg({
        'amount': ['count', 'mean', 'std'],
        'step': 'nunique'
    }).fillna(0)
    
    user_stats.columns = ['txn_count', 'avg_amount', 'std_amount', 'active_days']
    
    # Map back to original dataframe
    df = df.merge(user_stats, left_on='nameOrig', right_index=True, how='left')
    
    # Current transaction vs historical average
    df['ratio_current_to_avg'] = df['amount'] / (df['avg_amount'] + 1)
    
    # Merchant/Geographic pattern features
    print("  - Merchant/Geographic pattern features...")
    
    # Identify merchants vs customers
    df['is_merchant_dest'] = df['nameDest'].str.startswith('M').astype(int)
    
    # Destination diversity per user
    dest_stats = df.groupby('nameOrig')['nameDest'].agg(['nunique', 'count']).fillna(0)
    dest_stats.columns = ['unique_destinations', 'total_transactions']
    dest_stats['destination_diversity_ratio'] = dest_stats['unique_destinations'] / dest_stats['total_transactions']
    
    df = df.merge(dest_stats, left_on='nameOrig', right_index=True, how='left')
    
    # New destination flag (simplified)
    df['new_destination_flag'] = df.groupby(['nameOrig', 'nameDest']).cumcount() == 0
    df['new_destination_flag'] = df['new_destination_flag'].astype(int)
    
    # Simulate geographic features (for demonstration)
    print("  - Simulated geographic features...")
    np.random.seed(42)
    
    # Create consistent location mapping for destinations
    unique_dests = df['nameDest'].unique()
    dest_locations = pd.DataFrame({
        'nameDest': unique_dests,
        'dest_lat': np.random.uniform(-90, 90, len(unique_dests)),
        'dest_lon': np.random.uniform(-180, 180, len(unique_dests))
    })
    
    df = df.merge(dest_locations, on='nameDest', how='left')
    
    # Distance from most common location per user (simplified)
    user_common_location = df.groupby('nameOrig').agg({
        'dest_lat': 'median',
        'dest_lon': 'median'
    }).fillna(0)
    user_common_location.columns = ['user_common_lat', 'user_common_lon']
    
    df = df.merge(user_common_location, left_on='nameOrig', right_index=True, how='left')
    
    # Calculate distance from common location
    df['distance_from_common'] = np.sqrt(
        (df['dest_lat'] - df['user_common_lat'])**2 + 
        (df['dest_lon'] - df['user_common_lon'])**2
    )
    
    return df

def train_enhanced_model():
    """Train fraud detection model with enhanced features"""
    print("Starting Enhanced Fraud Detection Model Training")
    print("=" * 50)
    
    # Load dataset
    print("Loading PaySim dataset...")
    df = pd.read_csv('PS_20174392719_1491204439457_log.csv')
    print(f"Dataset shape: {df.shape}")
    
    # Use a manageable sample for demonstration
    print("Sampling 30,000 transactions for enhanced feature engineering...")
    # Sample with stratification manually to maintain fraud ratio
    fraud_samples = df[df['isFraud'] == 1].sample(n=min(1000, len(df[df['isFraud'] == 1])), random_state=42)
    normal_samples = df[df['isFraud'] == 0].sample(n=29000, random_state=42)
    df_sample = pd.concat([fraud_samples, normal_samples]).sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"Sample shape: {df_sample.shape}")
    print(f"Fraud rate in sample: {df_sample['isFraud'].mean():.4f}")
    
    # Original feature engineering
    print("\nCreating baseline features...")
    df_sample['balance_diff_orig'] = df_sample['oldbalanceOrg'] - df_sample['newbalanceOrig']
    df_sample['balance_diff_dest'] = df_sample['newbalanceDest'] - df_sample['oldbalanceDest']
    df_sample['amount_to_balance_ratio'] = df_sample['amount'] / (df_sample['oldbalanceOrg'] + 1)
    df_sample['orig_zero_balance'] = (df_sample['oldbalanceOrg'] == 0).astype(int)
    df_sample['dest_zero_balance'] = (df_sample['oldbalanceDest'] == 0).astype(int)
    
    # Encode transaction type
    type_encoder = LabelEncoder()
    df_sample['type_encoded'] = type_encoder.fit_transform(df_sample['type'])
    
    # Create enhanced features
    df_enhanced = create_optimized_features(df_sample)
    
    # Define feature set
    feature_columns = [
        # Original features
        'step', 'type_encoded', 'amount', 'oldbalanceOrg', 'newbalanceOrig',
        'oldbalanceDest', 'newbalanceDest', 'balance_diff_orig', 'balance_diff_dest',
        'amount_to_balance_ratio', 'orig_zero_balance', 'dest_zero_balance',
        # Enhanced transaction frequency features
        'txn_count', 'avg_amount', 'std_amount', 'active_days', 'ratio_current_to_avg',
        # Enhanced merchant/geographic features  
        'is_merchant_dest', 'unique_destinations', 'destination_diversity_ratio',
        'new_destination_flag', 'dest_lat', 'dest_lon', 'distance_from_common'
    ]
    
    # Prepare features and target
    X = df_enhanced[feature_columns].copy()
    y = df_enhanced['isFraud'].copy()
    
    # Handle missing values
    X = X.fillna(0)
    
    print(f"\nFeature matrix shape: {X.shape}")
    print(f"Target distribution: {y.value_counts()}")
    
    # Split data
    print("\nSplitting data (80/20 train/test)...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Scale features
    print("Scaling features...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train model
    print("Training Random Forest with enhanced features...")
    model = RandomForestClassifier(
        n_estimators=50,  # Reduced for faster training
        max_depth=12,
        min_samples_split=20,
        min_samples_leaf=10,
        random_state=42,
        n_jobs=-1,
        class_weight='balanced'
    )
    
    model.fit(X_train_scaled, y_train)
    
    # Evaluate model
    print("\nEvaluating enhanced model...")
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    
    print(f"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")
    
    # Feature importance analysis
    feature_importance = pd.DataFrame({
        'feature': feature_columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nTop 10 Most Important Features:")
    print(feature_importance.head(10))
    
    # Analyze new features specifically
    enhanced_features = [
        'txn_count', 'avg_amount', 'std_amount', 'ratio_current_to_avg',
        'is_merchant_dest', 'unique_destinations', 'destination_diversity_ratio',
        'new_destination_flag', 'distance_from_common'
    ]
    
    print("\nImportance of Enhanced Features:")
    enhanced_importance = feature_importance[feature_importance['feature'].isin(enhanced_features)]
    print(enhanced_importance)
    
    # Save model and preprocessing objects
    print("\nSaving enhanced model...")
    
    with open('fraud_model.pkl', 'wb') as f:
        pickle.dump(model, f)
    
    with open('scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    with open('encoder.pkl', 'wb') as f:
        pickle.dump(type_encoder, f)
    
    with open('feature_columns.pkl', 'wb') as f:
        pickle.dump(feature_columns, f)
    
    # Save feature implementation summary
    implemented_features = {
        'Transaction Frequency Features (Real)': [
            'txn_count', 'avg_amount', 'std_amount', 
            'active_days', 'ratio_current_to_avg'
        ],
        'Merchant Pattern Features (Real)': [
            'is_merchant_dest', 'unique_destinations', 
            'destination_diversity_ratio', 'new_destination_flag'
        ],
        'Geographic Features (Simulated)': [
            'dest_lat', 'dest_lon', 'distance_from_common'
        ]
    }
    
    with open('implemented_features.pkl', 'wb') as f:
        pickle.dump(implemented_features, f)
    
    # Print comprehensive summary
    print("\n" + "="*70)
    print("ENHANCED FRAUD DETECTION MODEL - IMPLEMENTATION SUMMARY")
    print("="*70)
    
    print(f"\nðŸ“Š Model Performance:")
    print(f"  â€¢ ROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")
    print(f"  â€¢ Training Samples: {len(X_train):,}")
    print(f"  â€¢ Test Samples: {len(X_test):,}")
    print(f"  â€¢ Total Features: {len(feature_columns)}")
    
    print(f"\nðŸš€ Successfully Implemented Features:")
    
    for category, features in implemented_features.items():
        print(f"\n{category}:")
        for feature in features:
            importance_val = feature_importance[feature_importance['feature'] == feature]['importance'].values
            if len(importance_val) > 0:
                status = "HIGH IMPACT" if importance_val[0] > 0.05 else "MODERATE IMPACT" if importance_val[0] > 0.02 else "LOW IMPACT"
                print(f"  âœ“ {feature:25} (importance: {importance_val[0]:.4f} - {status})")
            else:
                print(f"  âœ“ {feature}")
    
    print(f"\nðŸ’¾ Files Created:")
    print("  â€¢ fraud_model.pkl (enhanced model)")
    print("  â€¢ scaler.pkl")
    print("  â€¢ encoder.pkl") 
    print("  â€¢ feature_columns.pkl")
    print("  â€¢ implemented_features.pkl")
    
    print(f"\nðŸŽ¯ Feature Implementation Status:")
    print("  âœ“ Transaction Frequency Features: COMPLETED (5 features)")
    print("  âœ“ Merchant Pattern Features: COMPLETED (4 features)")  
    print("  âœ“ Geographic Features: COMPLETED (3 simulated features)")
    print("\nAll requested features have been successfully implemented!")
    
    return model, scaler, type_encoder, implemented_features

if __name__ == "__main__":
    train_enhanced_model()